---
title: "R Notebook"
output: html_notebook
---

### creating the corpus

```{r}
sms_raw = read.csv("C:/Users/hicks/OneDrive/Documents/UIS/Spring2022/CSC532/datasets/sms_spam.csv", stringsAsFactors = FALSE)
str(sms_raw)
```

converting the ham/spam to a factor
```{r}
sms_raw$type = factor(sms_raw$type)
t = table(sms_raw$type)
```
```{r}
prop.table(t)
```
there is 86% chance something is ham and 13% something is spam


```{r}
install.packages("tm")
library(tm)
```
tm is used in NLP - text mining.

```{r}
sms_corpus = VCorpus(VectorSource(sms_raw$text))
sms_corpus
```

```{r}
#inspect(sms_corpus)
inspect(sms_corpus[1])
```
a corpus is a collection of documents. Each document is a text/email message. 5559 row in the df = # of messages. inspect() returns list of list of the documents. use indexes to obtain the documents individually.
```{r}
as.character(sms_corpus[[1]])
```
use as.character() to show the contents of the document and use additional brackets to show the first element of the document.

```{r}
sapply(sms_corpus[1:5], as.character)
```
use sapply to get multiple documents. documents 1 through 5, and use as character to see their contents.

### text cleaning

```{r}
sms_corpus_clean = tm_map(sms_corpus, content_transformer(tolower))
```
convert the corpus all to lowercase

```{r}
sms_corpus_clean = tm_map(sms_corpus_clean, removeNumbers)
```
remove all the numbers

```{r}
sms_corpus_clean = tm_map(sms_corpus_clean, removeWords, stopwords())
```
remove all the words like "and", "the", "a" etc...

```{r}
#sms_corpus_clean = tm_map(sms_corpus_clean, removePunctuation)
replacePunctuation <- function(x) {
      gsub("[[:punct:]]+", " ", x)
      }
sms_corpus_clean = tm_map(sms_corpus_clean, content_transformer(replacePunctuation))
sapply(sms_corpus_clean[1:5], as.character)
```
remove punctuation

### text cleaning and standardization

```{r}
install.packages("SnowballC")
library(SnowballC)
sms_corpus_clean = tm_map(sms_corpus_clean, stemDocument)

```

removing "ing", "ed" etc
```{r}
sapply(sms_corpus_clean[13], as.character)
```

```{r}
sms_corpus_clean = tm_map(sms_corpus_clean, stripWhitespace)
sapply(sms_corpus_clean[1:3], as.character)
```
get rid of white space

```{r}
#sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
# tolower = TRUE,
# removeNumbers = TRUE,
# stopwords = TRUE,
# removePunctuation = TRUE,
# stemming = TRUE
# ))
sms_dtm = DocumentTermMatrix(sms_corpus_clean)
sms_dtm
```
tokenization with document term matrix, rows = docs. cols = words, 422826 non zero entries, 34039403 zero entries. High Sparsity. 

### train/test split and visualization wordcloud

```{r}
#doing a 75/25 split of the data
sms_dtm_train = sms_dtm[1:4169, ]
sms_dtm_test = sms_dtm[4170:5559, ]

#extract labels from the raw data
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
```

```{r}
install.packages("wordcloud")
library(wordcloud)
```

```{r}
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
```
word cloud of our data

```{r}
spam = subset(sms_raw, type == "spam")
ham = subset(sms_raw, type =="ham")
```
splitting the dataframe into two sub dataframes that just contain spam and just contain ham

```{r}
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
```

```{r}
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
```

### running naive bayes

```{r}
sms_freq_words = findFreqTerms(sms_dtm_train, 5)
sms_freq_words
```

filter words that appear less than 5 times

```{r}
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
```

selecting columns with only the frequent words

```{r}
convert_counts = function(x){x = ifelse(x>0, "Yes", "No")}

sms_train = apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_train = apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
```
MARGIN=1 applied to rows MARGIN=2 applied to columns
Applying convert counts function to data

```{r}
install.packages("e1071")
library(e1071)
```
```{r}

```

